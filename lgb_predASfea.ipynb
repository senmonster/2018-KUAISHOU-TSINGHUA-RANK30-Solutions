{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "6A10E485B5024A0B887F2BE63EB099B7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import heamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4F7F4138FB54C7B881F3138BE9F26C6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Regressor, Classifier\n",
    "from heamy.pipeline import ModelsPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D08940CE0A8441F0AED320E34BCCC877",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from rgf.sklearn import RGFClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, cross_val_score\n",
    "import gc\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import  StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from glmnet import LogitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_type": "code",
    "id": "EB8AF09109854069B8EC6FBCC80C8DA3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_dataset(datatype):\n",
    "    train = pd.read_hdf('train.h5', key='train').reset_index(drop=True)\n",
    "    train1_new_label = pd.read_hdf('new_label.h5', key='train1_new_label')\n",
    "    train2_new_label = pd.read_hdf('new_label.h5', key='train2_new_label')\n",
    "    train_new_label = pd.concat([train1_new_label, train2_new_label], axis=0, ignore_index=True)\n",
    "    del train_new_label['user_id']\n",
    "    train = pd.concat([train, train_new_label], axis=1)\n",
    "    X_test = pd.read_hdf('test.h5', key='test')\n",
    "    X_train = train\n",
    "    uid_tr = X_train['user_id']\n",
    "    uid_te = X_test['user_id']\n",
    "    del X_train['user_id'], X_test['user_id']\n",
    "    del train, train_new_label\n",
    "    gc.collect()\n",
    "    if datatype=='lgb':\n",
    "        print('final shape:', X_train.shape, X_test.shape,)\n",
    "        return X_train, X_test, uid_te\n",
    "    if datatype=='NN':\n",
    "        cate_col = ['register_type']\n",
    "        cont_col = [i for i in list(X_test) if i not in cate_col]\n",
    "        X_all_cont = pd.concat([X_train[cont_col], X_test[cont_col]], axis=0).reset_index(drop=True)\n",
    "        scaler = StandardScaler(with_mean=False )\n",
    "        scaler.fit(X_all_cont)\n",
    "        X_train_cont = pd.DataFrame(scaler.transform(X_train[cont_col]), columns=cont_col)\n",
    "        X_test_cont = pd.DataFrame(scaler.transform(X_test[cont_col]), columns=cont_col)\n",
    "        X_train_cont['register_type'] = X_train['register_type']\n",
    "        X_test_cont['register_type'] = X_test['register_type']\n",
    "        for i in ['label', 'fir_cre_day2final', 'fir_lau_day2final', 'fir_act_day2final', 'future_num']:\n",
    "            X_train_cont[i] = X_train[i]\n",
    "        X_train = pd.get_dummies(X_train_cont, columns=['register_type'])\n",
    "        X_test = pd.get_dummies(X_test_cont, columns=['register_type'])\n",
    "        print('final shape:', X_train.shape, X_test.shape)\n",
    "        return X_train, X_test, uid_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4DD05CC15454F8E9B93B815F0BCB028",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_lgb, X_test_lgb, uid_te = get_dataset('lgb')\n",
    "#X_train_NN, X_test_NN, uid_te = get_dataset('NN')\n",
    "#X_train_NNreg, X_test_NNreg, _= get_dataset('NNreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35A5D0CA01244CCC84C1DBDD370815E8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def targetASfea4lgb(X_train, X_test,target, all_target,  tartype):   \n",
    "    if tartype == 'binary':\n",
    "        X_train, Y_train = X_train.drop(all_target, axis=1), X_train[target]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X_train, Y_train)\n",
    "        train_preds = np.zeros(shape=(len(X_train), 1))\n",
    "        test_preds = np.zeros(shape=(len(X_test), 1))\n",
    "        for i , (tr_ix, va_ix) in enumerate(skf):\n",
    "            x_tr, y_tr, x_va, y_va = X_train.iloc[tr_ix, :], Y_train.iloc[tr_ix], X_train.iloc[va_ix, :], Y_train.iloc[va_ix]\n",
    "            gbm = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, \n",
    "                                learning_rate=0.06, \n",
    "                                n_estimators=5000,\n",
    "                                objective='binary', min_split_gain=0.0,\n",
    "                                min_child_weight=25,\n",
    "                                min_child_samples=10,\n",
    "                                subsample=0.8, subsample_freq=1,\n",
    "                                colsample_bytree=0.6, metric='auc',\n",
    "                                reg_alpha=0.0, reg_lambda=0.0,\n",
    "                                random_state=1, n_jobs=-1, silent=False) \n",
    "            lgb_tr = lgb.Dataset(x_tr, y_tr,  categorical_feature=['register_type'])\n",
    "            lgb_va = lgb.Dataset(x_va, y_va, reference=lgb_tr, categorical_feature=['register_type'])\n",
    "            model = lgb.train(gbm.get_params(), lgb_tr, early_stopping_rounds=50, valid_sets=lgb_va, verbose_eval=100)\n",
    "            train_preds[va_ix, 0] += model.predict(x_va)\n",
    "            test_preds[:, 0] += model.predict(X_test)\n",
    "        test_preds = test_preds / 10\n",
    "        train_fea = pd.DataFrame(train_preds, columns= ['lgbpred_' + target])\n",
    "        test_fea = pd.DataFrame(test_preds, columns= ['lgbpred_' + target])\n",
    "    if tartype == 'multi':\n",
    "        le = LabelEncoder()\n",
    "        X_train[target] = le.fit_transform(X_train[target])\n",
    "        X_train, Y_train = X_train.drop(all_target, axis=1), X_train[target]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X_train, Y_train)\n",
    "        #X_test\n",
    "        numclass = len(np.unique(Y_train))\n",
    "        train_preds = np.zeros(shape=(len(X_train), numclass))\n",
    "        test_preds = np.zeros(shape=(len(X_test), numclass))\n",
    "        for i , (tr_ix, va_ix) in enumerate(skf):\n",
    "            x_tr, y_tr, x_va, y_va = X_train.iloc[tr_ix, :], Y_train.iloc[tr_ix], X_train.iloc[va_ix, :], Y_train.iloc[va_ix]\n",
    "            gbm = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, \n",
    "                                learning_rate=0.06, \n",
    "                                n_estimators=3000,\n",
    "                                objective='multiclass', min_split_gain=0.0,\n",
    "                                num_class=numclass,\n",
    "                                min_child_weight=25,\n",
    "                                min_child_samples=10,\n",
    "                                subsample=0.8, subsample_freq=1,\n",
    "                                colsample_bytree=0.6, metric='multi_logloss',\n",
    "                                reg_alpha=0.0, reg_lambda=0.0,\n",
    "                                random_state=1, n_jobs=-1, silent=False) \n",
    "            lgb_tr = lgb.Dataset(x_tr, y_tr,  categorical_feature=['register_type'])\n",
    "            lgb_va = lgb.Dataset(x_va, y_va, reference=lgb_tr, categorical_feature=['register_type'])\n",
    "            model = lgb.train(gbm.get_params(), lgb_tr, early_stopping_rounds=50, valid_sets=lgb_va, verbose_eval=100)\n",
    "            train_preds[va_ix, :] += model.predict(x_va)\n",
    "            test_preds[:, :] += model.predict(X_test)\n",
    "        test_preds = test_preds / 10\n",
    "        train_fea = pd.DataFrame(train_preds, columns= ['lgbpred_' + target + str(i) for i in range(numclass)])\n",
    "        test_fea = pd.DataFrame(test_preds, columns= ['lgbpred_' + target + str(i) for i in range(numclass)])\n",
    "    if tartype == 'reg':\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=1).split(X_train)\n",
    "        gbm = lgb.LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=-1, \n",
    "                                learning_rate=0.06, \n",
    "                                n_estimators=5000,\n",
    "                                objective='regression_l2', min_split_gain=0.0,\n",
    "                                min_child_weight=25,\n",
    "                                min_child_samples=10,\n",
    "                                subsample=0.8, subsample_freq=1,\n",
    "                                colsample_bytree=0.6,\n",
    "                                reg_alpha=0.0, reg_lambda=0.0,  metric='l2_root',\n",
    "                                random_state=1, n_jobs=-1, silent=False)  \n",
    "        X_train, Y_train = X_train.drop(all_target, axis=1), X_train[target]\n",
    "        #X_test\n",
    "        train_preds = np.zeros(shape=(len(X_train), 1))\n",
    "        test_preds = np.zeros(shape=(len(X_test), 1))\n",
    "        for i , (tr_ix, va_ix) in enumerate(kf):\n",
    "            x_tr, y_tr, x_va, y_va = X_train.iloc[tr_ix, :], Y_train.iloc[tr_ix], X_train.iloc[va_ix, :], Y_train.iloc[va_ix]\n",
    "            lgb_tr = lgb.Dataset(x_tr, y_tr, categorical_feature=['register_type'])\n",
    "            lgb_va = lgb.Dataset(x_va, y_va, reference=lgb_tr, categorical_feature=['register_type'])\n",
    "            model = lgb.train(gbm.get_params(), lgb_tr, early_stopping_rounds=50, valid_sets=lgb_va, verbose_eval=100)\n",
    "            train_preds[va_ix, 0] += model.predict(x_va).ravel()\n",
    "            test_preds[:, 0] += model.predict(X_test).ravel()\n",
    "        test_preds = test_preds / 10\n",
    "        train_fea = pd.DataFrame(train_preds, columns= ['lgbpred_' + target])\n",
    "        test_fea = pd.DataFrame(test_preds, columns= ['lgbpred_' + target])\n",
    "    return train_fea, test_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33196D5373614553B9FD57D22D598447",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leak_multi = [i for i in X_train_lgb.columns if 'fir' in i]\n",
    "leak_cont = ['future_num']\n",
    "all_target = leak_multi + leak_cont + ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9D29E5170DED4D38B05BB84E94B69853",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls -lh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AA7BBB616994C108667253B7D24837C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_lgb_label, test_lgb_label = targetASfea4lgb(X_train_lgb,X_test_lgb, 'label',  all_target, 'binary') \n",
    "train_lgb_label.to_hdf('../input/leak.h5', key='train_lgb_label')\n",
    "test_lgb_label.to_hdf('../input/leak.h5', key='test_lgb_label')\n",
    "print('label done')\n",
    "train_lgb_leak4, test_lgb_leak4 = targetASfea4lgb(X_train_lgb,X_test_lgb, leak_cont[0],  all_target, 'reg') \n",
    "train_lgb_leak4.to_hdf('../input/leak.h5', key='train_lgb_leak4')\n",
    "test_lgb_leak4.to_hdf('../input/leak.h5', key='test_lgb_leak4')\n",
    "print('4 done')\n",
    "train_lgb_leak1, test_lgb_leak1 = targetASfea4lgb(X_train_lgb,X_test_lgb, leak_multi[0],  all_target, 'multi')\n",
    "train_lgb_leak1.to_hdf('../input/leak.h5', key='train_lgb_leak1')\n",
    "test_lgb_leak1.to_hdf('../input/leak.h5', key='test_lgb_leak1')\n",
    "print('1 done')\n",
    "train_lgb_leak2, test_lgb_leak2 = targetASfea4lgb(X_train_lgb,X_test_lgb, leak_multi[1],  all_target, 'multi') \n",
    "train_lgb_leak2.to_hdf('../input/leak.h5', key='train_lgb_leak2')\n",
    "test_lgb_leak2.to_hdf('../input/leak.h5', key='test_lgb_leak2')\n",
    "print('2 done')\n",
    "train_lgb_leak3, test_lgb_leak3 = targetASfea4lgb(X_train_lgb,X_test_lgb, leak_multi[2],  all_target, 'multi')\n",
    "train_lgb_leak3.to_hdf('../input/leak.h5', key='train_lgb_leak3')\n",
    "test_lgb_leak3.to_hdf('../input/leak.h5', key='test_lgb_leak3')\n",
    "print('3 done')\n",
    "\n",
    "train_lgb_label = pd.read_hdf('../input/leak.h5', key='train_lgb_label')\n",
    "test_lgb_label = pd.read_hdf('../input/leak.h5', key='test_lgb_label')\n",
    "\n",
    "train_lgb_leak4 = pd.read_hdf('../input/leak.h5', key='train_lgb_leak4')\n",
    "test_lgb_leak4 = pd.read_hdf('../input/leak.h5', key='test_lgb_leak4')\n",
    "\n",
    "train_lgb_leak1 = pd.read_hdf('../input/leak.h5', key='train_lgb_leak1')\n",
    "test_lgb_leak1 = pd.read_hdf('../input/leak.h5', key='test_lgb_leak1')\n",
    "\n",
    "train_lgb_leak2 = pd.read_hdf('../input/leak.h5', key='train_lgb_leak2')\n",
    "test_lgb_leak2 = pd.read_hdf('../input/leak.h5', key='test_lgb_leak2')\n",
    "\n",
    "train_lgb_leak3 = pd.read_hdf('../input/leak.h5', key='train_lgb_leak3')\n",
    "test_lgb_leak3 = pd.read_hdf('../input/leak.h5', key='test_lgb_leak3')\n",
    "\n",
    "lgbtrain_pred_fea = pd.concat([train_lgb_label, train_lgb_leak1, train_lgb_leak2, train_lgb_leak3, train_lgb_leak4], axis=1)\n",
    "lgbtest_pred_fea = pd.concat([test_lgb_label, test_lgb_leak1, test_lgb_leak2, test_lgb_leak3, test_lgb_leak4], axis=1)\n",
    "\n",
    "\n",
    "lgbtrain_pred_fea = lgbtrain_pred_fea.apply(pd.to_numeric, downcast='float')\n",
    "lgbtest_pred_fea = lgbtest_pred_fea.apply(pd.to_numeric, downcast='float')\n",
    "lgbtrain_pred_fea.to_hdf('pred_fea.h5', key='lgbtrain_pred_fea', complevel=9)\n",
    "lgbtest_pred_fea.to_hdf('pred_fea.h5', key='lgbtest_pred_fea', complevel=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以下未实现，效果不佳\n",
    "\"\"\"from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def NN_model(X_train, num_per_layer,  tartype, drop_per_layer=[.8, .6, .4]):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(num_per_layer[0], input_dim = X_train.shape[1], init = 'he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop_per_layer[0]))\n",
    "        \n",
    "    model.add(Dense(num_per_layer[1], init = 'he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(drop_per_layer[1]))\n",
    "    \n",
    "    model.add(Dense(num_per_layer[2], init = 'he_normal'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(drop_per_layer[2]))\n",
    "    \n",
    "    if tartype == 'binary':\n",
    "        model.add(Dense(1, init = 'he_normal', activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam')\n",
    "    if tartype == 'reg':\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='rmsprop', loss = root_mean_squared_error , metrics=['mae'])\n",
    "    if tartype == 'multi':\n",
    "        model.add(Dense(8, activation='softmax'))\n",
    "        model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def targetASfea4NN(X_train, X_test,target, all_target,  tartype):   \n",
    "    if tartype == 'binary':\n",
    "        X_train, Y_train = X_train.drop(all_target, axis=1), X_train[target]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X_train, Y_train)\n",
    "        train_preds = np.zeros(shape=(len(X_train), 1))\n",
    "        test_preds = np.zeros(shape=(len(X_test), 1))\n",
    "        for i , (tr_ix, va_ix) in enumerate(skf):\n",
    "            x_tr, y_tr, x_va, y_va = X_train.iloc[tr_ix, :], Y_train.iloc[tr_ix], X_train.iloc[va_ix, :], Y_train.iloc[va_ix]\n",
    "            model = NN_model(x_tr, [96,64,32], 'binary')\n",
    "            model.fit(x_tr, y_tr,epochs=100, batch_size=4096,\n",
    "                       validation_data=(x_va, y_va),\n",
    "                    callbacks= [EarlyStopping(monitor='val_loss',patience=3, verbose=30) ], verbose = 30)\n",
    "            train_preds[va_ix, 0] += model.predict(x_va).ravel()\n",
    "            test_preds[:, 0] += model.predict(X_test).ravel()\n",
    "        test_preds = test_preds / 5\n",
    "        train_fea = pd.DataFrame(train_preds, columns= ['pred_' + target])\n",
    "        test_fea = pd.DataFrame(test_preds, columns= ['pred_' + target])\n",
    "    if tartype == 'multi':\n",
    "        le = LabelEncoder()\n",
    "        X_train[target] = le.fit_transform(X_train[target])\n",
    "        X_train, Y_train = X_train.drop(all_target, axis=1), X_train[target]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X_train, Y_train)\n",
    "        numclass = len(np.unique(Y_train))\n",
    "        train_preds = np.zeros(shape=(len(X_train), numclass))\n",
    "        test_preds = np.zeros(shape=(len(X_test), numclass))\n",
    "        for i , (tr_ix, va_ix) in enumerate(skf):\n",
    "            x_tr, y_tr, x_va, y_va = X_train.iloc[tr_ix, :], Y_train.iloc[tr_ix], X_train.iloc[va_ix, :], Y_train.iloc[va_ix]\n",
    "            y_tr = np.array(pd.get_dummies(y_tr))\n",
    "            y_va = np.array(pd.get_dummies(y_va))\n",
    "            model = NN_model(x_tr, [96,64,32], 'multi')\n",
    "            model.fit(x_tr, y_tr,epochs=100, batch_size=4096,\n",
    "                       validation_data=(x_va, y_va),\n",
    "                    callbacks= [EarlyStopping(monitor='val_loss',patience=3, verbose=30) ], verbose = 30)\n",
    "            train_preds[va_ix, :] += model.predict(x_va)\n",
    "            test_preds[:, :] += model.predict(X_test)\n",
    "        test_preds = test_preds / 5\n",
    "        train_fea = pd.DataFrame(train_preds, columns= ['pred_' + target + str(i) for i in range(numclass)])\n",
    "        test_fea = pd.DataFrame(test_preds, columns= ['pred_' + target + str(i) for i in range(numclass)])\n",
    "    if tartype == 'reg':\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=1).split(X_train)\n",
    "        X_train, Y_train = X_train.drop(all_target, axis=1), X_train[target]\n",
    "        #X_test\n",
    "        train_preds = np.zeros(shape=(len(X_train), 1))\n",
    "        test_preds = np.zeros(shape=(len(X_test), 1))\n",
    "        for i , (tr_ix, va_ix) in enumerate(kf):\n",
    "            x_tr, y_tr, x_va, y_va = X_train.iloc[tr_ix, :], Y_train.iloc[tr_ix], X_train.iloc[va_ix, :], Y_train.iloc[va_ix]\n",
    "            model = NN_model(x_tr, [96,64,32], 'reg', drop_per_layer=[.5, .25, .15],)\n",
    "            model.fit(x_tr, y_tr,epochs=100, batch_size=4096,\n",
    "                       validation_data=(x_va, y_va),\n",
    "                    callbacks= [EarlyStopping(monitor='val_loss',patience=3, verbose=2) ], verbose = 2)\n",
    "            train_preds[va_ix, 0] += model.predict(x_va).ravel()\n",
    "            test_preds[:, 0] += model.predict(X_test).ravel()\n",
    "        test_preds = test_preds/5\n",
    "        train_fea = pd.DataFrame(train_preds, columns= ['pred_' + target])\n",
    "        test_fea = pd.DataFrame(test_preds, columns= ['pred_' + target])\n",
    "    return train_fea, test_fea\n",
    "\n",
    "# 16 .45\n",
    "train_NN_label, test_NN_label = targetASfea4NN(X_train_NN,X_test_NN, 'label',  all_target, 'binary') \n",
    "train_NN_leak4, test_NN_leak4 = targetASfea4NN(X_train_NN,X_test_NN, leak_cont[0],  all_target, 'reg') \n",
    "train_NN_leak1, test_NN_leak1 = targetASfea4NN(X_train_NN,X_test_NN, leak_multi[0],  all_target, 'multi') \n",
    "train_NN_leak2, test_NN_leak2 = targetASfea4NN(X_train_NN,X_test_NN, leak_multi[1],  all_target, 'multi') \n",
    "train_NN_leak3, test_NN_leak3 = targetASfea4NN(X_train_NN,X_test_NN, leak_multi[2],  all_target, 'multi') \n",
    "\n",
    "NNtrain_pred_fea = pd.concat([train_NN_label, train_NN_leak1, train_NN_leak2, train_NN_leak3, train_NN_leak4], axis=1)\n",
    "NNtest_pred_fea = pd.concat([test_NN_label, test_NN_leak1, test_NN_leak2, test_NN_leak3, test_NN_leak4], axis=1)\n",
    "NNtrain_pred_fea.to_hdf('../input/pred_fea.h5', key='NNtrain_pred_fea')\n",
    "NNtest_pred_fea.to_hdf('../input/pred_fea.h5', key='NNtest_pred_fea')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
