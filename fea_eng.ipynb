{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4B4AABA3D8E745BD87E9875634FB0643",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "#from scipy.stats import stats\n",
    "#import lightgbm as lgb\n",
    "%matplotlib inline\n",
    "import gc\n",
    "#from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "#from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6E38B551EE0458180E578B9E7BF5661",
    "mdEditEnable": false
   },
   "source": [
    "# Memory optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false,
    "id": "A22415BC3F644CE98E7C5712F78675CC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.7 s, sys: 7.39 s, total: 32.1 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "register = pd.read_table('../raw_data/user_register_log.txt', sep='\\t', header=None, \n",
    "             names=['user_id', 'register_day',\n",
    "                    'register_type', 'device_type']).sort_values(\n",
    "                                            by=['user_id', 'register_day'])\n",
    "register =  register.apply(pd.to_numeric, downcast='unsigned')\n",
    "launch = pd.read_table('../raw_data/app_launch_log.txt', sep='\\t', header=None,\n",
    "                      names=['user_id', 'launch_day']).sort_values(by=['user_id',\n",
    "                                                                     'launch_day'])\n",
    "launch =  launch.apply(pd.to_numeric, downcast='unsigned')\n",
    "video = pd.read_table('../raw_data/video_create_log.txt', sep='\\t', header=None,\n",
    "                     names=['user_id', 'create_day']).sort_values(by=['user_id',\n",
    "                                                                     'create_day'])\n",
    "video =  video.apply(pd.to_numeric, downcast='unsigned')\n",
    "activity = pd.read_table('../raw_data/user_activity_log.txt', sep='\\t', header=None,\n",
    "                        names=['user_id', 'act_day', 'act_page', 'video_id', 'author_id',\n",
    "                              'act_type']).sort_values(by=['user_id', 'act_day'])\n",
    "activity =  activity.apply(pd.to_numeric, downcast='unsigned')\n",
    "register.reset_index(drop=True).to_pickle('../tmp_data/register.pkl',)\n",
    "launch.reset_index(drop=True).to_pickle('../tmp_data/launch.pkl',)\n",
    "video.reset_index(drop=True).to_pickle('../tmp_data/create.pkl',)\n",
    "activity.reset_index(drop=True).to_pickle('../tmp_data/activity.pkl')\n",
    "del register, launch, video, activity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1B0A0452F860442EAF8D14FC99653565",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!tar -jcvf <要生成的压缩文件名> <要压缩的文件> # 压缩\n",
    "!tar -jxvf <要解压缩的文件> # 解压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F17DF9B646F498F926C454A522D3AFD",
    "mdEditEnable": false
   },
   "source": [
    "# load preprocess raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92D8737DB87F4383871BB1B19263EB46",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "register = pd.read_pickle('/home/kesci/input/register.pkl',)\n",
    "launch = pd.read_pickle('/home/kesci/input/launch.pkl',)\n",
    "create = pd.read_pickle('/home/kesci/input/create.pkl',)\n",
    "activity = pd.read_pickle('/home/kesci/input/activity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3D5567F24EA4F66A966F551B8E9535A",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('register user:', len(register['user_id']),\n",
    "      'launch user:', len(np.unique(launch['user_id'])),\n",
    "      'create user:', len(np.unique(create['user_id'])),\n",
    "      'activity user:', len(np.unique(activity['user_id'])),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "886E8CEED7B944EF8868380D13BB60CD",
    "mdEditEnable": false
   },
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60217C98A6B2433598CEAAD893AD03BA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cut_data(register, create, launch, activity, s_day_user, e_day_user, s_day_fea, e_day_fea, *label_cut_day):\n",
    "    train_register = register[(s_day_user <= register['register_day']) & \n",
    "         (register['register_day'] <= e_day_user)].reset_index(drop=True)\n",
    "    train_create = create[(s_day_fea <= create['create_day']) & \n",
    "         (create['create_day'] <= e_day_fea)].reset_index(drop=True)\n",
    "    train_launch = launch[(s_day_fea <= launch['launch_day']) & \n",
    "         (launch['launch_day'] <= e_day_fea)].reset_index(drop=True)\n",
    "    train_activity = activity[(s_day_fea <= activity['act_day']) & \n",
    "         (activity['act_day'] <= e_day_fea)].reset_index(drop=True)\n",
    "    train_activity['author_equal_user'] = train_activity['user_id'] == train_activity['author_id']\n",
    "    if len(label_cut_day) == 2:\n",
    "        s_day_label = label_cut_day[0]\n",
    "        e_day_label = label_cut_day[1]\n",
    "        test_register = register[(s_day_label <= register['register_day']) & \n",
    "             (register['register_day'] <= e_day_label)]\n",
    "        test_create = create[(s_day_label <= create['create_day']) & \n",
    "             (create['create_day'] <= e_day_label)]\n",
    "        test_launch = launch[(s_day_label <= launch['launch_day']) & \n",
    "             (launch['launch_day'] <= e_day_label)]\n",
    "        test_activity = activity[(s_day_label <= activity['act_day']) & \n",
    "             (activity['act_day'] <= e_day_label)]\n",
    "        test_user_id = set(test_register.user_id.values.tolist() + \\\n",
    "                        test_create.user_id.values.tolist() + \\\n",
    "                          test_launch.user_id.values.tolist() + \\\n",
    "                            test_activity.user_id.values.tolist())\n",
    "        train_register['label'] = [1 if i in test_user_id else 0 for i in train_register.user_id]\n",
    "    return train_register, train_create, train_launch, train_activity       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7711AB7809B64E1D8781D17F58E91735",
    "mdEditEnable": false
   },
   "source": [
    "# 生成基础统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     91
    ],
    "id": "63DBDFBC5EE544948F036919E32C2359",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#每个表各自生成基础统计特征 6，7，13，14，21，22，27，28日的数据会出现一次突增\n",
    "def get_register_feature(train_register, label_day):\n",
    "    # 注册类型\n",
    "    # 设备类型\n",
    "    # 'register_type', 'device_type', 'register_day_cut_max_day',\n",
    "    train_register['register_day_to_label'] = label_day - train_register['register_day']\n",
    "    return train_register\n",
    "\n",
    "def get_create_feature(group, label_day):\n",
    "    feature = pd.Series()\n",
    "    feature['create_count'] = len(group) # 创建视频次数    \n",
    "    # 一天最多创建多少视频\n",
    "    feature['max_create_for_oneday'] = max(Counter(group['create_day']).values())\n",
    "    # 所有创建视频日与打标签日之间的距离的最大值，最小值，方差,均值 ,峰度 ,偏度\n",
    "    dis2lab = label_day - np.unique(group['create_day'])\n",
    "    feature['max_create_day2label']= np.max(dis2lab)\n",
    "    feature['min_create_day2label']= np.min(dis2lab)\n",
    "    feature['mean_create_day2label'] = np.mean(dis2lab)\n",
    "    feature['median_create_day2label'] = np.median(dis2lab)\n",
    "    feature['std_create_day2label'] = np.std(dis2lab)\n",
    "    # 所有创建视频日与注册日之间的距离的最大值，最小值，方差,均值 ,峰度 ,偏度\n",
    "    dis2reg = np.unique(group['create_day']) - list(group['register_day'])[0]\n",
    "    feature['max_create_day2regis'] = np.max(dis2reg)\n",
    "    feature['min_create_day2regis'] = np.min(dis2reg)\n",
    "    feature['mean_create_day2regis'] = np.mean(dis2reg)\n",
    "    feature['median_create_day2regis'] = np.median(dis2reg)\n",
    "    feature['std_create_day2regis'] = np.std(dis2reg)\n",
    " \n",
    "    #gap\n",
    "    gap4cre = dis2reg - dis2lab\n",
    "    feature['max_gap4cre'] = np.max(gap4cre)\n",
    "    feature['min_gap4cre'] = np.min(gap4cre)\n",
    "    feature['mean_gap4cre'] = np.mean(gap4cre)\n",
    "    feature['median_gap4cre'] = np.median(gap4cre)\n",
    "    feature['std_gap4cre'] = np.std(gap4cre)\n",
    " \n",
    "\n",
    "    feature['mean_daily_create'] = len(group) / (len(np.unique(group['create_day'])))\n",
    "    diff_day = np.diff(np.unique(group['create_day']))\n",
    "    diff_list = ['1' if v == 1 else ' ' for v in diff_day]\n",
    "    if len(diff_day) != 0: # 连续创建视频的最大天数,创建视频间隔的均值，方差，最大值，最小值，\n",
    "        if '1' in diff_list:\n",
    "            feature['max_contin_create_days'] = 1 +  max([len(x) for\\\n",
    "                                                     x in ''.join(diff_list).split()])\n",
    "        feature['create_day_diff_mean'] = np.mean(diff_day)\n",
    "        feature['create_day_diff_std'] = np.std(diff_day)\n",
    "        feature['create_day_diff_max'] = np.max(diff_day)\n",
    "        feature['create_day_diff_min'] = np.min(diff_day)\n",
    "    return feature\n",
    "\n",
    "def get_launch_feature(group, label_day):\n",
    "    feature = pd.Series()\n",
    "    feature['launch_count'] = len(group) # 启动次数\n",
    "    # 所有启动日与打标签日之间的距离的最大值，最小值，方差,均值\n",
    "    dis2lab = label_day - group['launch_day']\n",
    "    feature['max_launch_day2label']= np.max(dis2lab)\n",
    "    feature['min_launch_day2label']= np.min(dis2lab)\n",
    "    feature['mean_launch_day2label'] = np.mean(dis2lab)\n",
    "    feature['median_launch_day2label'] = np.median(dis2lab)\n",
    "    feature['std_launch_day2label'] = np.std(dis2lab)\n",
    "\n",
    "    \n",
    "    # 所有启动日与注册日之间的距离的最大值，最小值，方差,均值 ,峰度 ,偏度\n",
    "    dis2reg = group['launch_day'] - list(group['register_day'])[0]\n",
    "    feature['max_launch_day2regis'] = np.max(dis2reg)\n",
    "    feature['min_launch_day2regis'] = np.min(dis2reg)\n",
    "    feature['mean_launch_day2regis'] = np.mean(dis2reg)\n",
    "    feature['median_launch_day2regis'] = np.median(dis2reg)\n",
    "    feature['std_launch_day2regis'] = np.std(dis2reg)\n",
    "   \n",
    "    #gap\n",
    "    gap4lau = dis2reg - dis2lab\n",
    "    feature['max_gap4lau'] = np.max(gap4lau)\n",
    "    feature['min_gap4lau'] = np.min(gap4lau)\n",
    "    feature['mean_gap4lau'] = np.mean(gap4lau)\n",
    "    feature['median_gap4lau'] = np.median(gap4lau)\n",
    "    feature['std_gap4lau'] = np.std(gap4lau)\n",
    "  \n",
    "    \n",
    "    diff_day = np.diff(group['launch_day'])\n",
    "    diff_list = ['1' if v == 1 else ' ' for v in diff_day]\n",
    "    if len(diff_day) != 0: # 连续启动app的最大天数, 启动间隔的均值，方差，最大值，最小值, 峰度， 偏度\n",
    "        if '1' in diff_list:\n",
    "            feature['max_contin_launch_days'] = 1 + max([len(x) for x in ''.join(diff_list).split()])\n",
    "        feature['launch_day_diff_mean'] = np.mean(diff_day)\n",
    "        feature['launch_day_diff_std'] = np.std(diff_day)\n",
    "        feature['launch_day_diff_max'] = np.max(diff_day)\n",
    "        feature['launch_day_diff_min'] = np.min(diff_day)\n",
    "     \n",
    "    return feature\n",
    "\n",
    "def get_activity_feature(group, label_day): \n",
    "    feature = pd.Series()\n",
    "    feature['activity_count'] = len(group)\n",
    "    feature['sum_author_equal_user'] = group['author_equal_user'].sum() #用户等于视频作者用户的次数\n",
    "    feature['max_repeat_video'] = max(Counter(group['video_id']).values()) #用户重复操作某个视频的最大次数\n",
    "    feature['max_repeat_author'] = max(Counter(group['author_id']).values()) #用户关于某个作者最多发生多少次操作行为    feature['mean_daily_act'] = len(group) / len(np.unique(group['act_day']))\n",
    "    \n",
    "    # 一天最多发生多少次活动\n",
    "    feature['max_act_for_oneday'] = max(Counter(group['act_day']).values())\n",
    "    feature['mean_daily_act'] = len(group) / len(np.unique(group['act_day']))\n",
    "\n",
    "    # 所有发生行为日与打标签日之间的距离的最大值，最小值，方差,均值，峰度， 偏度\n",
    "    dis2lab = label_day - np.unique(group['act_day'])\n",
    "    feature['max_act_day2label']= np.max(dis2lab)\n",
    "    feature['min_act_day2label']= np.min(dis2lab)\n",
    "    feature['mean_act_day2label'] = np.mean(dis2lab)\n",
    "    feature['median_act_day2label'] = np.median(dis2lab)\n",
    "    feature['std_act_day2label'] = np.std(dis2lab)\n",
    "   \n",
    "    \n",
    "    # 所有发生行为日与注册日之间的距离的最大值，最小值，方差,均值 ,峰度 ,偏度\n",
    "    dis2reg = np.unique(group['act_day']) - list(group['register_day'])[0]\n",
    "    feature['max_act_day2regis'] = np.max(dis2reg)\n",
    "    feature['min_act_day2regis'] = np.min(dis2reg)\n",
    "    feature['mean_act_day2regis'] = np.mean(dis2reg)\n",
    "    feature['median_act_day2regis'] = np.median(dis2reg)\n",
    "    feature['std_act_day2regis'] = np.std(dis2reg)\n",
    " \n",
    "    #gap\n",
    "    gap4act = dis2reg - dis2lab\n",
    "    feature['max_gap4act'] = np.max(gap4act)\n",
    "    feature['min_gap4act'] = np.min(gap4act)\n",
    "    feature['mean_gap4act'] = np.mean(gap4act)\n",
    "    feature['median_gap4act'] = np.median(gap4act)\n",
    "    feature['std_gap4act'] = np.std(gap4act)\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    diff_day = np.diff(np.unique(group['act_day']))\n",
    "    diff_list = ['1' if v == 1 else ' ' for v in diff_day]\n",
    "    if len(diff_day) != 0: # 连续发生行为的最大天数，发生行为天数间隔的均值，方差，最大值，最小值，\n",
    "        if '1' in diff_list:\n",
    "            feature['max_contin_act_days'] = 1 + max([len(x) for x in ''.join(diff_list).split()])\n",
    "        feature['act_day_diff_mean'] = np.mean(diff_day)\n",
    "        feature['act_day_diff_std'] = np.std(diff_day)\n",
    "        feature['act_day_diff_max'] = np.max(diff_day)\n",
    "        feature['act_day_diff_min'] = np.min(diff_day)\n",
    "     \n",
    "    feature['0_actpage_count'] = np.sum(group['act_page'] == 0)\n",
    "    feature['1_actpage_count'] = np.sum(group['act_page'] == 1)\n",
    "    feature['2_actpage_count'] = np.sum(group['act_page'] == 2)\n",
    "    feature['3_actpage_count'] = np.sum(group['act_page'] == 3)\n",
    "    feature['4_actpage_count'] = np.sum(group['act_page'] == 4)\n",
    "    feature['0_actpage_count_div_sum'] = np.sum(group['act_page'] == 0) / len(group['act_page'])\n",
    "    feature['1_actpage_count_div_sum'] = np.sum(group['act_page'] == 1) / len(group['act_page'])\n",
    "    feature['2_actpage_count_div_sum'] = np.sum(group['act_page'] == 2) / len(group['act_page'])\n",
    "    feature['3_actpage_count_div_sum'] = np.sum(group['act_page'] == 3) / len(group['act_page'])\n",
    "    feature['4_actpage_count_div_sum'] = np.sum(group['act_page'] == 4) / len(group['act_page'])\n",
    "    \n",
    "    feature['0_actype_count'] = np.sum(group['act_type'] == 0)\n",
    "    feature['1_actype_count'] = np.sum(group['act_type'] == 1)\n",
    "    feature['2_actype_count'] = np.sum(group['act_type'] == 2)\n",
    "    feature['3_actype_count'] = np.sum(group['act_type'] == 3)\n",
    "    feature['4_actype_count'] = np.sum(group['act_type'] == 4)\n",
    "    feature['5_actype_count'] = np.sum(group['act_type'] == 5)\n",
    "    feature['0_actype_count_div_sum'] = np.sum(group['act_type'] == 0) / len(group['act_type'])\n",
    "    feature['1_actype_count_div_sum'] = np.sum(group['act_type'] == 1) / len(group['act_type'])\n",
    "    feature['2_actype_count_div_sum'] = np.sum(group['act_type'] == 2) / len(group['act_type'])\n",
    "    feature['3_actype_count_div_sum'] = np.sum(group['act_type'] == 3) / len(group['act_type'])\n",
    "    feature['4_actype_count_div_sum'] = np.sum(group['act_type'] == 4) / len(group['act_type'])\n",
    "    feature['5_actype_count_div_sum'] = np.sum(group['act_type'] == 5) / len(group['act_type'])\n",
    "    return feature\n",
    "def deal_feature(train_register, train_create, train_launch, train_act, label_day):\n",
    "    train = get_register_feature(train_register, label_day)    \n",
    "    act_author = pd.DataFrame()\n",
    "    act_author['user_id'] = train_act['author_id']\n",
    "    act_author['size'] = 1\n",
    "    reg_author = pd.merge(train[['user_id']], act_author, on='user_id', how='left')\n",
    "    impact = reg_author.groupby('user_id')['size'].apply(np.sum).values\n",
    "    train['user_impact'] = impact\n",
    "    print('register表特征提取完毕')\n",
    "    \n",
    "    \n",
    "    train_create = pd.merge(train_create, train_register[['user_id', 'register_day']],\n",
    "                  on='user_id', how='left')\n",
    "    cre_feature = train_create.groupby('user_id', sort=True).apply(get_create_feature, label_day)\n",
    "    cre_feature = cre_feature.reset_index().pivot(index='user_id', columns='level_1', values=0).rename_axis(None, axis=1)\n",
    "    cre_feature = cre_feature.reset_index()\n",
    "    train = pd.merge(train, pd.DataFrame(cre_feature),\n",
    "                      on='user_id', how='left')\n",
    "    print('create表特征提取完毕')\n",
    "\n",
    "    train_launch = pd.merge(train_launch, train_register[['user_id', 'register_day']],\n",
    "                  on='user_id', how='left')\n",
    "    lau_feature = train_launch.groupby('user_id', sort=True).apply(get_launch_feature, label_day)\n",
    "    lau_feature = lau_feature.reset_index().pivot(index='user_id', columns='level_1', values=0).rename_axis(None, axis=1)\n",
    "    lau_feature = lau_feature.reset_index()\n",
    "    train = pd.merge(train, pd.DataFrame(lau_feature),\n",
    "                      on='user_id', how='left')\n",
    "    print('launch表特征提取完毕')\n",
    "    \n",
    "    train_act = pd.merge(train_act, train_register[['user_id', 'register_day']],\n",
    "                  on='user_id', how='left')\n",
    "    act_feature = train_act.groupby('user_id', sort=True).apply(get_activity_feature, label_day)\n",
    "    act_feature = act_feature.reset_index().pivot(index='user_id', columns='level_1', values=0).rename_axis(None, axis=1)\n",
    "    act_feature = act_feature.reset_index()\n",
    "    train = pd.merge(train, pd.DataFrame(act_feature),\n",
    "                      on='user_id', how='left')\n",
    "    print('activity表特征提取完毕')\n",
    "    if 'register_day' in train.columns:\n",
    "        del train['register_day']\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95D20CFEB62145528D1E1BC56BD6392C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#--------------------------------------------cut1---------------------------------------------------------\n",
    "train_reg1, train_cre1, train_lau1, train_act1 = cut_data(register, create, launch, activity,1, 16, 1, 16, 17, 23)\n",
    "train1 = deal_feature(train_reg1, train_cre1, train_lau1, train_act1, 17)\n",
    "train1.to_hdf('../tmp_data/cut_train1.h5', key='train1', complevel=9, complib='zlib')\n",
    "del train_reg1, train_cre1, train_lau1, train_act1, train1\n",
    "gc.collect()\n",
    "\n",
    "train_reg2, train_cre2, train_lau2, train_act2 = cut_data(register, create, launch, activity,1, 23, 8, 23, 24, 30)\n",
    "train2 = deal_feature(train_reg2, train_cre2, train_lau2, train_act2, 24)\n",
    "train2.to_hdf('../tmp_data/cut_train2.h5', key='train2', complevel=9, complib='zlib')\n",
    "print('train done')\n",
    "del train_reg2, train_cre2, train_lau2, train_act2, train2\n",
    "gc.collect()\n",
    "\n",
    "test_reg, test_cre, test_lau, test_act = cut_data(register, create, launch, activity,1, 30, 15, 30)\n",
    "test = deal_feature(test_reg, test_cre, test_lau, test_act, 31)\n",
    "test.to_hdf('../tmp_data/cut_test.h5', key='test', complevel=9, complib='zlib')\n",
    "del test_reg, test_cre, test_lau, test_act, test\n",
    "del register, create, launch, activity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B8847645165431E8AA842503033C7F4",
    "mdEditEnable": false
   },
   "source": [
    "# add_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F9625702BC24DE0BF5C94CC77152C17",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#每个表各自生成基础统计特征 6，7，13，14，21，22，27，28日的数据会出现一次突增\n",
    "def add_register_feature(train_register):\n",
    "    # 注册类型\n",
    "    # 设备类型\n",
    "    # 'register_type', 'device_type', 'register_day_cut_max_day',\n",
    "    return train_register[['user_id']]\n",
    "\n",
    "def add_create_feature(group):\n",
    "    feature = pd.Series()\n",
    "    # 一天最少创建多少视频\n",
    "    feature['min_create_for_oneday'] = min(list(Counter(group['create_day']).values()))\n",
    "    feature['std_create_daily'] = np.std(list(Counter(group['create_day']).values()))\n",
    "    return feature\n",
    "\n",
    "def add_activity_feature(group): \n",
    "    feature = pd.Series()\n",
    "    feature['unique_video'] = len(np.unique(group['video_id']))\n",
    "    feature['unique_author'] = len(np.unique(group['author_id']))\n",
    "    \n",
    "    # 一天最少发生多少次活动\n",
    "    feature['min_act_for_oneday'] = min(list(Counter(group['act_day']).values()))\n",
    "    feature['std_act_daily'] = np.std(list(Counter(group['act_day']).values()))\n",
    "    \n",
    "    count_video = list(Counter(group['video_id']).values())\n",
    "    feature['min_video_num'] = np.min(count_video)\n",
    "    feature['mean_video_num'] = np.mean(count_video)\n",
    "    feature['std_video_num'] = np.std(count_video)\n",
    "    \n",
    "    count_author = list(Counter(group['author_id']).values())\n",
    "    feature['min_author_num'] = np.min(count_author)\n",
    "    feature['mean_author_num'] = np.mean(count_author)\n",
    "    feature['std_author_num'] = np.std(count_author)\n",
    "    \n",
    "    # user操作不同作者不同视屏个数的统计值\n",
    "    diff_video = list(Counter(group.drop_duplicates(subset=['author_id', 'video_id'])['author_id']).values())\n",
    "    feature['min_video_per_author'] = np.min(diff_video)\n",
    "    feature['max_video_per_author'] = np.max(diff_video)\n",
    "    feature['mean_video_per_author'] = np.mean(diff_video)\n",
    "    feature['std_video_per__author'] = np.std(diff_video)\n",
    "    return feature\n",
    "def add_feature(train_register, train_create, train_act):\n",
    "    train = add_register_feature(train_register)\n",
    "    print('add register表特征提取完毕')\n",
    "    \n",
    "    cre_feature = train_create.groupby('user_id', sort=True).apply(add_create_feature)\n",
    "    cre_feature = cre_feature.reset_index()\n",
    "    train = pd.merge(train, cre_feature,\n",
    "                      on='user_id', how='left')\n",
    "    print('add create表特征提取完毕')\n",
    "\n",
    "    act_feature = train_act.groupby('user_id', sort=True).apply(add_activity_feature)\n",
    "    act_feature = act_feature.reset_index()\n",
    "    train = pd.merge(train, act_feature,\n",
    "                      on='user_id', how='left')\n",
    "    print('add activity表特征提取完毕')\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8C09B2FAE704EB989D86F40A33CD0AA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_reg1, train_cre1, train_lau1, train_act1 = cut_data(register, create, launch, activity,1, 16, 1, 16, 17, 23)\n",
    "add_train1 = add_feature(train_reg1, train_cre1, train_act1)\n",
    "add_train1.to_hdf('../tmp_data/ADD.f5', key='add_train1', complevel=9)\n",
    "del train_reg1, train_cre1, train_lau1, train_act1 , add_train1\n",
    "gc.collect()\n",
    "\n",
    "train_reg2, train_cre2, train_lau2, train_act2 = cut_data(register, create, launch, activity,1, 23, 8, 23, 24, 30)\n",
    "add_train2 = add_feature(train_reg2, train_cre2, train_act2)\n",
    "add_train2.to_hdf('../tmp_data/ADD.f5', key='add_train2', complevel=9)\n",
    "del train_reg2, train_cre2, train_lau2, train_act2, add_train2\n",
    "gc.collect()\n",
    "\n",
    "test_reg, test_cre, test_lau, test_act = cut_data(register, create, launch, activity,1, 30, 15, 30)\n",
    "add_test = add_feature(test_reg, test_cre, test_act)\n",
    "add_test.to_hdf('../tmp_data/ADD.f5', key='add_test', complevel=9)\n",
    "del test_reg, test_cre, test_lau, test_act, add_test\n",
    "\n",
    "\n",
    "del register, create, launch, activity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6934213AAF354FFD8981DA2CAAFA5CAA",
    "mdEditEnable": false
   },
   "source": [
    "# 生成与label距离的活跃个数特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28FB4906C3A24A258771FD453A9E630D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_dis_cre_fea(group, label_day):\n",
    "    feature = pd.Series()\n",
    "    group['cre_to_lab'] = label_day - group['create_day']\n",
    "    feature['count_cre_within_1_day_from_label'] = np.sum(group['cre_to_lab'] <= 1)\n",
    "    feature['count_cre_within_3_day_from_label'] = np.sum(group['cre_to_lab'] <= 3)\n",
    "    feature['count_cre_within_5_day_from_label'] = np.sum(group['cre_to_lab'] <= 5)\n",
    "    feature['count_cre_within_7_day_from_label'] = np.sum(group['cre_to_lab'] <= 7)\n",
    "    feature['count_cre_within_9_day_from_label'] = np.sum(group['cre_to_lab'] <= 9)\n",
    "    feature['count_cre_within_11_day_from_label'] = np.sum(group['cre_to_lab'] <= 11)\n",
    "    feature['count_cre_within_13_day_from_label'] = np.sum(group['cre_to_lab'] <= 13)\n",
    "    return feature\n",
    "\n",
    "def get_dis_lau_fea(group, label_day):\n",
    "    feature = pd.Series()\n",
    "    group['lau_to_lab'] = label_day - group['launch_day']\n",
    "    feature['count_lau_within_1_day_from_label'] = np.sum(group['lau_to_lab'] <= 1)\n",
    "    feature['count_lau_within_3_day_from_label'] = np.sum(group['lau_to_lab'] <= 3)\n",
    "    feature['count_lau_within_5_day_from_label'] = np.sum(group['lau_to_lab'] <= 5)\n",
    "    feature['count_lau_within_7_day_from_label'] = np.sum(group['lau_to_lab'] <= 7)\n",
    "    feature['count_lau_within_9_day_from_label'] = np.sum(group['lau_to_lab'] <= 9)\n",
    "    feature['count_lau_within_11_day_from_label'] = np.sum(group['lau_to_lab'] <= 11)\n",
    "    feature['count_lau_within_13_day_from_label'] = np.sum(group['lau_to_lab'] <= 13)\n",
    "    return feature\n",
    "\n",
    "def get_dis_act_fea(group, label_day):\n",
    "    feature = pd.Series()\n",
    "    group['act_to_lab'] = label_day - group['act_day']\n",
    "    feature['count_act_within_1_day_from_label'] = np.sum(group['act_to_lab'] <= 1)\n",
    "    feature['count_act_within_3_day_from_label'] = np.sum(group['act_to_lab'] <= 3)\n",
    "    feature['count_act_within_5_day_from_label'] = np.sum(group['act_to_lab'] <= 5)\n",
    "    feature['count_act_within_7_day_from_label'] = np.sum(group['act_to_lab'] <= 7)\n",
    "    feature['count_act_within_9_day_from_label'] = np.sum(group['act_to_lab'] <= 9)\n",
    "    feature['count_act_within_11_day_from_label'] = np.sum(group['act_to_lab'] <= 11)\n",
    "    feature['count_act_within_13_day_from_label'] = np.sum(group['act_to_lab'] <= 13)\n",
    "    return feature\n",
    "\n",
    "def dis_all_fea(train_register, train_create, train_launch, train_act, label_day):\n",
    "    dis_cre_fea = train_create.groupby('user_id', sort=True).apply(get_dis_cre_fea, (label_day,))\n",
    "    dis_cre_fea = dis_cre_fea.reset_index()\n",
    "\n",
    "    dis_lau_fea = train_launch.groupby('user_id', sort=True).apply(get_dis_lau_fea, (label_day,))\n",
    "    dis_lau_fea = dis_lau_fea.reset_index()\n",
    "\n",
    "    dis_act_fea = train_act.groupby('user_id', sort=True).apply(get_dis_act_fea, (label_day,))\n",
    "    dis_act_fea = dis_act_fea.reset_index()\n",
    "    \n",
    "    dis_all_fea = train_register[['user_id']].merge(dis_cre_fea, on='user_id', how='left').\\\n",
    "                                    merge(dis_lau_fea, on='user_id', how='left').\\\n",
    "                                    merge(dis_act_fea, on='user_id', how='left')\n",
    "    return dis_all_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6C2884ACE8944368A030566EC93C070",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#--------------------------------------------cut1---------------------------------------------------------train_reg1, train_cre1, train_lau1, train_act1 = cut_data(register, create, launch, activity,1, 16, 1, 16, 17, 23)\n",
    "train_reg1, train_cre1, train_lau1, train_act1 = cut_data(register, create, launch, activity,1, 16, 1, 16, 17, 23)\n",
    "dis_train1 = dis_all_fea(train_reg1, train_cre1, train_lau1, train_act1, 17)\n",
    "dis_train1.to_hdf('../tmp_data/cut1.h5', key='dis_train1', complevel=9, complib='zlib')\n",
    "del train_reg1, train_cre1, train_lau1, train_act1, dis_train1\n",
    "gc.collect()\n",
    "\n",
    "train_reg2, train_cre2, train_lau2, train_act2 = cut_data(register, create, launch, activity,1, 23, 8, 23, 24, 30)\n",
    "dis_train2 = dis_all_fea(train_reg2, train_cre2, train_lau2, train_act2, 24)\n",
    "dis_train2.to_hdf('../tmp_data/cut1.h5', key='dis_train2', complevel=9, complib='zlib')\n",
    "del train_reg2, train_cre2, train_lau2, train_act2, dis_train2\n",
    "gc.collect()\n",
    "\n",
    "test_reg, test_cre, test_lau, test_act = cut_data(register, create, launch, activity,1, 30, 15, 30)\n",
    "dis_test = dis_all_fea(test_reg, test_cre, test_lau, test_act, 31)\n",
    "dis_test.to_hdf('../tmp_data/cut1.h5', key='dis_test', complevel=9, complib='zlib')\n",
    "\n",
    "del test_reg, test_cre, test_lau, test_act, dis_test\n",
    "del register, create, launch, activity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95DFD757E48E468683C0380FCB52A278",
    "mdEditEnable": false
   },
   "source": [
    "# add hcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10E614C550D54BD68BBC2F9B7A55146D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add hcc inter fea\n",
    "def binner(key, alldata,  maxbins = 101, na = -100, percent_per_bin = 1):\n",
    "    raw_column = alldata[key].copy()\n",
    "    raw_column.fillna(na, inplace = True)\n",
    "    akey = raw_column[raw_column != na]\n",
    "    count = len(akey.unique())\n",
    "    if count < maxbins:\n",
    "        return (alldata[key], None)\n",
    "    try:\n",
    "        bins = np.unique(np.percentile(akey, np.arange(0, 100, percent_per_bin)))\n",
    "        # Add a bin for NA\n",
    "        if np.min(raw_column) == na:\n",
    "            bins = np.insert(bins, 0, na + 1)\n",
    "        count = len(bins)\n",
    "        binned_column = np.digitize(raw_column, bins)\n",
    "        binned_column = [key + \"_\" + str(x) for x in binned_column]\n",
    "        return (binned_column)\n",
    "    except:\n",
    "        return (raw_column)        \n",
    "def add_cate(alldata, num2cate=[], tobin=[]):\n",
    "    if tobin != []:\n",
    "        for col in tobin:\n",
    "            alldata['binned_' + col] = binner(col, alldata)\n",
    "        bin = [i for i in alldata.columns if 'binned' in i]\n",
    "    else:\n",
    "        bin = []\n",
    "    if num2cate != []:\n",
    "        for col in num2cate:\n",
    "            alldata[col] = alldata[col].map(lambda x: col + '_' +  str(x))\n",
    "    cate = ['device_type', 'register_type', ]\n",
    "    \n",
    "    all_cate = cate + num2cate + bin\n",
    "    #two-way interactions\n",
    "    combi = list(combinations(all_cate, 2))\n",
    "    inter2fea = []\n",
    "    for (i, j) in combi:\n",
    "        ij = i + \"_\" + j\n",
    "        inter2fea.append(ij)\n",
    "        alldata[ij] = alldata[i].astype(str) + \"_\" + alldata[j].astype(str)\n",
    "      \n",
    "    # three-way interactions\n",
    "    combi_three_way = list(combinations(all_cate, 3))\n",
    "    inter3fea = []\n",
    "    if len(all_cate) >= 3: \n",
    "        for (i, j, k) in combi_three_way:\n",
    "            ijk = i + \"_\" + j + \"_\" + k\n",
    "            inter3fea.append(ijk)\n",
    "            alldata[i + \"_\" + j + \"_\" + k] = alldata[i].astype(str) + \"_\" + alldata[j].astype(str) + \"_\" + alldata[k].astype(str)\n",
    "    \n",
    "    all_cate = all_cate + inter2fea + inter3fea   \n",
    "    \"\"\"trans_cols = [i for i in cate_cols if i not in cate]\n",
    "    cate_map = []\n",
    "    for i in trans_cols:\n",
    "        key = alldata[i].value_counts().keys()\n",
    "        value = range(len(key))\n",
    "        dic = dict(zip(key, value))\n",
    "        cate_map.append(dic)\n",
    "    \n",
    "    for i in range(len(trans_cols)):\n",
    "        alldata[trans_cols[i]] = alldata[trans_cols[i]].map(cate_map[i])\"\"\"\n",
    "    return alldata, all_cate\n",
    "#-----------------------get hcc fea------------------------------------\n",
    "def hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n",
    "    \"\"\"\n",
    "    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n",
    "    Classification and Prediction Problems\" by Daniele Micci-Barreca\n",
    "    \"\"\"\n",
    "    hcc_name = \"_\".join([\"hcc\", variable, target])\n",
    "    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n",
    "    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n",
    "    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n",
    "    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n",
    "    np.random.seed(1011)\n",
    "    if r_k: \n",
    "        df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n",
    "    if update_df is None:\n",
    "        update_df = test_df\n",
    "    if hcc_name not in update_df.columns: \n",
    "        update_df[hcc_name] = np.nan\n",
    "    update_df.update(df)\n",
    "    return \n",
    "def add_hcc(train, test, cate_cols):\n",
    "    skf = StratifiedKFold(10)\n",
    "    prior = train['label'].mean()\n",
    "    attributes = product(set(cate_cols), [('label', prior)])\n",
    "    for variable, (target, prior) in attributes:\n",
    "        hcc_encode(train, test, variable, target, prior, k=5, r_k=None)\n",
    "        for tra, tes in skf.split(np.zeros(len(train)), train['label']):\n",
    "            hcc_encode(train.iloc[tra], train.iloc[tes], variable, target, prior, k=5, r_k=0.01, update_df=train)\n",
    "    obejct_col = [f for f in train.columns if train[f].dtype == \"object\"]\n",
    "    train.drop(obejct_col, axis=1, inplace=True)\n",
    "    test.drop(obejct_col, axis=1, inplace=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "329A5FEBDBFE44B98E5E7E1BF707B97B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train1 = pd.read_hdf('../tmp_data/cut_train1.h5', key='train1')\n",
    "#dis_train1 = pd.read_hdf('dis.h5', key='dis_train1')\n",
    "#train1 = train1.merge(dis_train1, how='left', on='user_id')\n",
    "train2 = pd.read_hdf('../tmp_data/cut_train2.h5', key='train2')\n",
    "#dis_train2 = pd.read_hdf('dis.h5', key='dis_train2')\n",
    "#train2 = train2.merge(dis_train2, how='left', on='user_id')\n",
    "train = pd.concat([train1, train2], ignore_index=True, axis=0)\n",
    "test = pd.read_hdf('../tmp_data/cut_test.h5', key='test')\n",
    "#dis_test = pd.read_hdf('dis.h5', key='dis_test')\n",
    "#test = test.merge(dis_test, how='left', on='user_id')\n",
    "del train1, train2\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F50739B1A3364EE08AEF454F0821F043",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = train.set_index('user_id')\n",
    "test = test.set_index('user_id')\n",
    "train[\"train_flag\"] = 1\n",
    "test[\"train_flag\"] = 0\n",
    "test[\"label\"] = -1\n",
    "alldata = pd.concat([train, test], axis = 0)\n",
    "\n",
    "#num2cate =  ['min_of_launch_days_to_label', 'max_launch_day_cut_regis']\n",
    "#tobin = ['count_act_within_1_day_from_label', '1_action_count_div_sum', '3_page_count_div_sum',\n",
    "#        'mean_of_launch_days_to_label', '1_page_count_div_sum']\n",
    "alldata, cate_cols= add_cate(alldata, \\\n",
    " num2cate=[], tobin=[])\n",
    "print(cate_cols)\n",
    "print('done')\n",
    "train_processed = alldata[alldata[\"train_flag\"] == 1].copy()\n",
    "test_processed = alldata[alldata[\"train_flag\"] == 0].copy()\n",
    "train_processed, test_processed = add_hcc(train_processed, test_processed, cate_cols)\n",
    "X_train = train_processed.drop(['label', 'train_flag'], axis=1)\n",
    "Y_train = train_processed['label']\n",
    "X_test = test_processed.drop(['train_flag', 'label'], axis=1)    \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "436FE6232406433DA82F4605A0654C90",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.reset_index()[['user_id', 'hcc_device_type_label', 'hcc_register_type_label', 'hcc_device_type_register_type_label']].to_hdf('hcc.h5', key='hcc_train', complevel=9, complib='zlib')\n",
    "X_test.reset_index()[['user_id', 'hcc_device_type_label', 'hcc_register_type_label','hcc_device_type_register_type_label']].to_hdf('hcc.h5', key='hcc_test', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38DCE0F8897F42DA8D52C22086FE33B3",
    "mdEditEnable": false
   },
   "source": [
    "# add2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3E8497C64A924B5F81D04E1A331CD7A0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_register_feature(train_register):\n",
    "    # 注册类型\n",
    "    # 设备类型\n",
    "    # 'register_type', 'device_type', 'register_day_cut_max_day',\n",
    "    return train_register[['user_id']]\n",
    "\n",
    "def add_activity_feature2(group): \n",
    "    feature = pd.Series()\n",
    "    act_days = np.unique(group['act_day'])\n",
    "    uni_vd = np.unique(group['video_id'])\n",
    "    uni_au = np.unique(group['author_id'])\n",
    "    \n",
    "    day_vd = {} #字典存放user 某天 操作的视频列表\n",
    "    for i in act_days:\n",
    "        day_vd[i] = group[group['act_day'] == i]['video_id']\n",
    "        \n",
    "    day_max_vd = {} # 字典存放user 每天最多操作同一视频多少次\n",
    "    day_mean_vd = {} # 字典存放user 每天平均重复操作每个视频多少次\n",
    "    for i in act_days:\n",
    "        day_max_vd[i] = max(list(Counter(day_vd[i]).values()))\n",
    "        day_mean_vd[i] = np.mean(list(Counter(day_vd[i]).values()))\n",
    "    feature['max_act4vd_alldaymax'] = np.max(list(day_max_vd.values())) # 所有天中单日最多操作同一视频多少次\n",
    "    feature['mean_act4vd_alldaymax'] = np.mean(list(day_max_vd.values())) # 所有天中平均单日最多操作同一视频多少次\n",
    "    feature['max_act4vd_alldaymean'] = np.max(list(day_mean_vd.values())) # 最多单日平均重复操作每个视频多少次\n",
    "    feature['mean_act4vd_alldaymean'] = np.mean(list(day_mean_vd.values())) # 平均每天平均操作同一视频多少次\n",
    "    \n",
    "    vd_day_count = {} #字典存放用户操作的所有video共在多少天出现过\n",
    "    for i in uni_vd:\n",
    "        vd_day_count[i] = 0\n",
    "        for j in day_vd.keys():\n",
    "            if i in day_vd[j]:\n",
    "                vd_day_count[i] += 1\n",
    "    feature['max_count4_per_vd'] = max(list(vd_day_count.values()))\n",
    "    feature['mean_count4_per_vd'] = np.mean(list(vd_day_count.values()))\n",
    "    \n",
    "                                                \n",
    "    day_au = {} #字典存放user 某天 操作的作者列表\n",
    "    for i in act_days:\n",
    "        day_au[i] = group[group['act_day'] == i]['author_id']\n",
    "                                                \n",
    "    day_max_au = {} # 字典存放user 每天最多操作同一作者多少次\n",
    "    day_mean_au = {} # 字典存放user 每天平均重复操作每个作者多少次\n",
    "    for i in act_days:\n",
    "        day_max_au[i] = max(list(Counter(day_au[i]).values()))\n",
    "        day_mean_au[i] = np.mean(list(Counter(day_au[i]).values()))                                                \n",
    "    feature['max_act4au_alldaymax'] = np.max(list(day_max_au.values())) # 所有天中单日最多操作同一作者多少次\n",
    "    feature['mean_act4au_alldaymax'] = np.mean(list(day_max_au.values())) # 所有天中平均单日最多操作同一作者多少次\n",
    "    feature['max_act4au_alldaymean'] = np.max(list(day_mean_au.values())) # 最多单日平均重复操作每个作者多少次\n",
    "    feature['mean_act4au_alldaymean'] = np.mean(list(day_mean_au.values())) # 平均每天平均操作同一作者多少次\n",
    "    \n",
    "    au_day_count = {} #  #字典存放用户操作的所有author共在多少天出现过\n",
    "    for i in uni_au:\n",
    "        au_day_count[i] = 0\n",
    "        for j in day_au.keys():\n",
    "            if i in day_au[j]:\n",
    "                au_day_count[i] += 1\n",
    "    feature['max_count4_per_au'] = max(list(au_day_count.values()))\n",
    "    feature['mean_count4_per_au'] = np.mean(list(au_day_count.values()))\n",
    "    return feature\n",
    "def add_feature2(train_register, train_create, train_act):\n",
    "    train = add_register_feature(train_register)\n",
    "    print('add2 register表特征提取完毕')\n",
    "    \n",
    "    act_feature = train_act.groupby('user_id', sort=True).apply(add_activity_feature2)\n",
    "    act_feature = act_feature.reset_index()\n",
    "    train = pd.merge(train, act_feature,\n",
    "                      on='user_id', how='left')\n",
    "    print('add2 activity表特征提取完毕')\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73B415925F5B4A8E993401AB8B1DEF24",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_reg1, train_cre1, train_lau1, train_act1 = cut_data(register, create, launch, activity,1, 16, 1, 16, 17, 23)\n",
    "add2_train1 = add_feature2(train_reg1, train_cre1, train_act1)\n",
    "add2_train1.to_hdf('../tmp_data/ADD.f5', key='add2_train1', complevel=9)\n",
    "del train_reg1, train_cre1, train_lau1, train_act1, add2_train1\n",
    "gc.collect()\n",
    "\n",
    "train_reg2, train_cre2, train_lau2, train_act2 = cut_data(register, create, launch, activity,1, 23, 8, 23, 24, 30)\n",
    "add2_train2 = add_feature2(train_reg2, train_cre2, train_act2)\n",
    "add2_train2.to_hdf('../tmp_data/ADD.f5', key='add2_train2', complevel=9)\n",
    "del train_reg2, train_cre2, train_lau2, train_act2, , add2_train2\n",
    "gc.collect()\n",
    "\n",
    "test_reg, test_cre, test_lau, test_act = cut_data(register, create, launch, activity,1, 30, 15, 30)\n",
    "add2_test = add_feature2(test_reg, test_cre, test_act)\n",
    "add2_test.to_hdf('../tmp_data/ADD.f5', key='add2_test', complevel=9)\n",
    "del test_reg, test_cre, test_lau, test_act, add2_test\n",
    "\n",
    "\n",
    "del register, create, launch, activity\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E250279FD2B4985A96DC3D3692047CE",
    "mdEditEnable": false
   },
   "source": [
    "# put my feature together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A529B2E48E99460F9311B78116A311F5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A1DECF33B3F458EB019651B5352AED5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ad = pd.HDFStore('ADD.f5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFEBBDAF00344BA08BE08773216974D6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "train1 = pd.read_hdf('../tmp_data/cut_train1.h5', key='train1')\n",
    "dis_train1 = pd.read_hdf('../tmp_data/dis.h5', key='dis_train1')\n",
    "train1 = train1.merge(dis_train1, how='left', on='user_id')\n",
    "add_train1 = pd.read_hdf('../tmp_data/ADD.f5', key='add_train1')\n",
    "train1 = train1.merge(add_train1, how='left', on='user_id')\n",
    "add2_train1 = pd.read_hdf('../tmp_data/ADD.f5', key='add2_train1')\n",
    "train1 = train1.merge(add2_train1, how='left', on='user_id')\n",
    "\n",
    "train2 = pd.read_hdf('../tmp_data/cut_train2.h5', key='train2')\n",
    "dis_train2 = pd.read_hdf('../tmp_data/dis.h5', key='dis_train2')\n",
    "train2 = train2.merge(dis_train2, how='left', on='user_id')\n",
    "add_train2 = pd.read_hdf('../tmp_data/ADD.f5', key='add_train2')\n",
    "train2 = train2.merge(add_train2, how='left', on='user_id')\n",
    "add2_train2 = pd.read_hdf('../tmp_data/ADD.f5', key='add2_train2')\n",
    "train2 = train2.merge(add2_train2, how='left', on='user_id')\n",
    "\n",
    "train = pd.concat([train1, train2], axis=0).set_index('user_id')\n",
    "\n",
    "hcc_train = pd.read_hdf('../tmp_data/hcc.h5', key='hcc_train').set_index('user_id')\n",
    "train = pd.concat([train, hcc_train], axis=1).reset_index()\n",
    "\n",
    "test = pd.read_hdf('../tmp_data/cut_test.h5', key='test')\n",
    "dis_test = pd.read_hdf('../tmp_data/dis.h5', key='dis_test')\n",
    "test = test.merge(dis_test, how='left', on='user_id')#.set_index('user_id')\n",
    "add_test = pd.read_hdf('../tmp_data/ADD.f5', key='add_test')\n",
    "test = test.merge(add_test, how='left', on='user_id')#.set_index('user_id')\n",
    "add2_test = pd.read_hdf('../tmp_data/ADD.f5', key='add2_test')\n",
    "test = test.merge(add2_test, how='left', on='user_id').set_index('user_id')\n",
    "\n",
    "hcc_test = pd.read_hdf('../tmp_data/hcc.h5', key='hcc_test').set_index('user_id')\n",
    "test = pd.concat([test, hcc_test], axis=1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"lgbtrain_pred_fea = pd.read_hdf('../input/pred_fea.h5', key='lgbtrain_pred_fea')\n",
    "lgbtest_pred_fea = pd.read_hdf('../input/pred_fea.h5', key='lgbtest_pred_fea')\n",
    "train = pd.concat([train, lgbtrain_pred_fea], axis=1)\n",
    "test = pd.concat([test, lgbtest_pred_fea], axis=1)\"\"\"\n",
    "\n",
    "del train1, train2\n",
    "del add_train1, add_train2, add_test\n",
    "del add2_train1, add2_train2, add2_test\n",
    "del hcc_train, hcc_test, dis_train1, dis_train2, dis_test\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "for i in list(test):\n",
    "    if any(t in i for t in ['gap4', '2regis']):\n",
    "        train[i] = train[i].fillna(-1)\n",
    "        test[i] = test[i].fillna(-1)\n",
    "    elif '2label' in i:\n",
    "        train[i]= train[i].fillna(17)\n",
    "        test[i] = test[i].fillna(17)\n",
    "    else:\n",
    "        train[i] = train[i].fillna(0)\n",
    "        test[i] = test[i].fillna(0)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AB36F7D6D004E7D864F0340A107796D",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_int = train.select_dtypes(include=['int'])\n",
    "train_int =  train_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "train_float = train.select_dtypes(include=['float'])\n",
    "train_float = train_float.apply(lambda x : np.around(x, 5)).apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "train[train_int.columns] = train_int\n",
    "train[train_float.columns] = train_float\n",
    "del train_int, train_float\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1CD85899E974C6AB21E0E2D9BA620D1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_int = test.select_dtypes(include=['int'])\n",
    "test_int =  test_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "test_float = test.select_dtypes(include=['float'])\n",
    "test_float = test_float.apply(lambda x : np.around(x, 5)).apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "test[test_int.columns] = test_int\n",
    "test[test_float.columns] = test_float\n",
    "del test_int, test_float\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B41B8C9658684EAA8FAE75CD693C36A1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.to_hdf('../tmp_data/train.h5', key='train', complevel=9)\n",
    "test.to_hdf('../tmp_data/test.h5', key='test', complevel=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
